{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3uOMJVp8FCt"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==2.16.0 huggingface-hub==0.34.0 scikit-learn jenga pandas numpy setuptools nltk category_encoders ftfy cleanlab seaborn -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y35SJL4N8FCu"
      },
      "source": [
        "## Create Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vltfd6l6n9cl",
        "outputId": "8e2d756a-120f-47bb-8d0c-63fe3d61aa45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder structure created\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "if 'notebook' in current_dir:\n",
        "    BASE_DIR = os.path.dirname(current_dir)\n",
        "else:\n",
        "    BASE_DIR = current_dir\n",
        "\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "# Create folder structure\n",
        "os.makedirs('results/', exist_ok=True)\n",
        "os.makedirs('figures/', exist_ok=True)\n",
        "os.makedirs('data/', exist_ok=True)\n",
        "\n",
        "RESULTS_DIR = os.path.join(BASE_DIR, \"results/\")\n",
        "FIGURES_DIR = os.path.join(BASE_DIR, \"figures/\")\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data/\")\n",
        "\n",
        "print(f\"Folder structure created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_HkNMsht9U"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkwMVhYO8FCw"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgpndYwjhve6"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error\n",
        "from scipy.stats import ttest_rel, ks_2samp, norm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "import ast\n",
        "from datasets import load_dataset\n",
        "\n",
        "import sklearn\n",
        "sklearn.set_config(enable_metadata_routing=True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usg3ptg38FCw"
      },
      "source": [
        "### Import corruption & cleaning modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80881ET_iKrB",
        "outputId": "9696ab40-19d6-4ae0-8fb6-ecf85fcb1749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imported corruption & cleaning modules\n"
          ]
        }
      ],
      "source": [
        "sys.path.append(os.path.join(BASE_DIR, \"python_scripts/\"))\n",
        "\n",
        "# Import unified corruption functions\n",
        "import corruptions as corrupt\n",
        "\n",
        "# Import unified cleaning functions\n",
        "import cleaning_functions as clean\n",
        "\n",
        "print(\"Imported corruption & cleaning modules\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTFxNlUKiQhm"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-6rsIre8FCx"
      },
      "source": [
        "### Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AojJTf0g8FCx"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.20\n",
        "SAMPLES_PER_CLASS = 20000\n",
        "EXPECTED_TOTAL = SAMPLES_PER_CLASS * 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eH-mvIp8FCx"
      },
      "source": [
        "### Preprocessing and Model Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4JEFrLxiT1i"
      },
      "outputs": [],
      "source": [
        "# feature extraction: Dual TF-IDF (word + char n-grams)\n",
        "text_features = FeatureUnion([\n",
        "    (\"word_tfidf\", TfidfVectorizer(\n",
        "        tokenizer=str.split,\n",
        "        lowercase=False,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=5,\n",
        "        max_df=0.8,\n",
        "        max_features=10000,\n",
        "        sublinear_tf=True,\n",
        "    )),\n",
        "    (\"char_tfidf\", TfidfVectorizer(\n",
        "        analyzer=\"char\",            # Character-level n-grams\n",
        "        ngram_range=(3, 4),\n",
        "        min_df=10,\n",
        "        max_features=5000,\n",
        "        sublinear_tf=True,\n",
        "    ))\n",
        "])\n",
        "\n",
        "# logistic regression model with sample weights support\n",
        "def build_model():\n",
        "    return Pipeline([\n",
        "        (\"features\", text_features),\n",
        "        (\"clf\", LogisticRegression(\n",
        "            max_iter=2000,\n",
        "            C=1.0,\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=-1\n",
        "        ).set_fit_request(sample_weight=True))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7g64QOMibUe"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9X8IEHGicbh"
      },
      "outputs": [],
      "source": [
        "def clear_memory():\n",
        "    _ = gc.collect()\n",
        "\n",
        "@contextmanager\n",
        "def timer():\n",
        "    # context manager to time a block of code\n",
        "    start = time.perf_counter()\n",
        "    yield lambda: time.perf_counter() - start\n",
        "\n",
        "def prepare_baseline_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # prepare clean baseline with row_id for fixed splits\n",
        "    df = df.copy()\n",
        "    df[\"text\"] = df[\"text\"].astype(str)\n",
        "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"label\"])\n",
        "    df[\"label\"] = df[\"label\"].astype(int)\n",
        "    df = df.reset_index(drop=True)\n",
        "    df[\"row_id\"] = df.index\n",
        "    return df\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    # compute all ML metrics\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "        \"mae\": float(mean_absolute_error(y_true, np.clip(y_pred, 1, 5)))\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTcjkxS4i8jh"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVNXC_1_8FCx",
        "outputId": "abb811cc-f456-422f-b6de-72442c12e52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the raw dataset...\n",
            "Loaded: 701,528 rows\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading the raw dataset...\")\n",
        "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True)\n",
        "\n",
        "df_raw = dataset[\"full\"].to_pandas()\n",
        "if 'rating' in df_raw.columns:\n",
        "  df_raw = df_raw.rename(columns={'rating': 'label'})\n",
        "print(f\"Loaded: {len(df_raw):,} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGyA0xlRi_Ch",
        "outputId": "7d723b4f-f4e0-496c-8c9f-c5eeb396b69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled to Balanced Pool: 100,000 total rows\n",
            "Class Distribution: {1.0: 20000, 2.0: 20000, 3.0: 20000, 4.0: 20000, 5.0: 20000}\n"
          ]
        }
      ],
      "source": [
        "df_raw = df_raw.groupby('label', group_keys=False).apply(\n",
        "    lambda x: x.sample(n=min(len(x), SAMPLES_PER_CLASS), random_state=9)\n",
        ")\n",
        "\n",
        "baseline_data_path = os.path.join(DATA_DIR, \"baseline_data.csv\")\n",
        "df_raw.to_csv(baseline_data_path, index=False)\n",
        "print(f\"Sampled to Balanced Pool: {len(df_raw):,} total rows\")\n",
        "print(f\"Class Distribution: {df_raw['label'].value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUlRDdHyiey3"
      },
      "source": [
        "## Training and Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D6jP1vjih2Y"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(df_train, df_test, train_ids, test_ids):\n",
        "    # train and evaluate the model on given train and test splits\n",
        "    # returns metrics and stats\n",
        "    train_valid = df_train[df_train[\"row_id\"].isin(train_ids)].copy()\n",
        "    test_valid = df_test[df_test[\"row_id\"].isin(test_ids)].copy()\n",
        "\n",
        "    if len(train_valid) < 20 or len(test_valid) < 10:\n",
        "        return None, {\"error\": \"insufficient_data\"}\n",
        "\n",
        "    X_train = train_valid[\"text\"].astype(str).values\n",
        "    y_train = pd.to_numeric(train_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
        "    X_test = test_valid[\"text\"].astype(str).values\n",
        "    y_test = pd.to_numeric(test_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
        "\n",
        "    # sample Weights (required for Model aware cleaning)\n",
        "    # check if 'sample_weight' column exists in the training split\n",
        "    weights = None\n",
        "    if \"sample_weight\" in train_valid.columns:\n",
        "        weights = train_valid[\"sample_weight\"].values\n",
        "\n",
        "    model = build_model()\n",
        "    if weights is not None:\n",
        "        # we pass sample_weight to the fit method\n",
        "        model.fit(X_train, y_train, sample_weight=weights)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # basic ML evaluation metrics\n",
        "    metrics = compute_metrics(y_test, y_pred)\n",
        "    stats = {\n",
        "        \"train_total\": len(train_valid),\n",
        "        \"train_valid\": len(train_valid),\n",
        "        \"test_total\": len(test_valid),\n",
        "        \"test_valid\": len(test_valid),\n",
        "    }\n",
        "\n",
        "    del model, X_train, y_train, X_test, y_test, train_valid, test_valid\n",
        "    clear_memory()\n",
        "\n",
        "    return metrics, stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CVMEKyAw44i"
      },
      "source": [
        "## Running the Model on Baseline Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nspE5Bw8FCy"
      },
      "source": [
        "### Prepare baseline data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU8jgv7D2rkR"
      },
      "outputs": [],
      "source": [
        "df_baseline = prepare_baseline_data(df_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx9jPqcW8FCy"
      },
      "source": [
        "### Run the model on baseline data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R5xemP6i0t_",
        "outputId": "afeb143b-2b38-4152-e46b-50941e7ce708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split: 80,000 train / 20,000 test\n",
            "Training the model on baseline data...\n",
            "Baseline Accuracy: 0.5152 (51.52%)\n",
            "MAE: 0.6400\n",
            "Evaluation took: 177.09 seconds\n"
          ]
        }
      ],
      "source": [
        "# create fixed train/test split\n",
        "df_train_clean, df_test_clean = train_test_split(\n",
        "    df_baseline, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df_baseline[\"label\"]\n",
        ")\n",
        "train_ids = set(df_train_clean[\"row_id\"].tolist())\n",
        "test_ids = set(df_test_clean[\"row_id\"].tolist())\n",
        "print(f\"Split: {len(df_train_clean):,} train / {len(df_test_clean):,} test\")\n",
        "\n",
        "# train and evaluate on baseline data\n",
        "print(\"Training the model on baseline data...\")\n",
        "with timer() as t:\n",
        "    metrics_base, stats_base = train_and_evaluate(df_baseline, df_baseline, train_ids, test_ids)\n",
        "baseline_acc = metrics_base['accuracy']\n",
        "print(f\"Baseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
        "print(f\"MAE: {metrics_base['mae']:.4f}\")\n",
        "print(f\"Evaluation took: {t():.2f} seconds\")\n",
        "\n",
        "# save baseline results\n",
        "baseline_info = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"dataset_size\": len(df_baseline),\n",
        "    \"train_size\": len(train_ids),\n",
        "    \"test_size\": len(test_ids),\n",
        "    \"baseline_accuracy\": baseline_acc,\n",
        "    \"eval_time\": t(),\n",
        "    **metrics_base\n",
        "}\n",
        "\n",
        "baseline_results_path = os.path.join(RESULTS_DIR, \"baseline_results.csv\")\n",
        "pd.DataFrame([baseline_info]).to_csv(baseline_results_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKwk8Ul9jDLh"
      },
      "source": [
        "### Corrupt the data -> Train and Evaluate -> Clean -> Re-train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UKDJrO68FCy"
      },
      "outputs": [],
      "source": [
        "def run_batch_experiments(batch_id, corruption_list):\n",
        "    \"\"\"Run corruption, cleaning, and evaluation for a batch of experiments.\"\"\"\n",
        "    batch_results = []\n",
        "    for idx, (exp_id, corrupt_func, kwargs) in enumerate(corruption_list, start=1):\n",
        "        print(\"\\n----------------------------------------------------------\")\n",
        "        print(f\"[{idx}/{len(corruption_list)}] {exp_id} in {batch_id}\")\n",
        "        print(\"----------------------------------------------------------\\n\")\n",
        "\n",
        "        # Corrupt\n",
        "        with timer() as t_corrupt:\n",
        "            corrupted_df = corrupt_func(df_baseline.copy(), **kwargs)\n",
        "        corrupt_time = t_corrupt()\n",
        "        print(f\"Corruption took: {corrupt_time:.2f} seconds\")\n",
        "\n",
        "        corrupted_data_baseline = os.path.join(DATA_DIR, f\"{exp_id}_data.csv\")\n",
        "        corrupted_df.to_csv(corrupted_data_baseline, index=False)\n",
        "\n",
        "        # cleaning strategies\n",
        "        cleaning_strategies = [\n",
        "            (\"0_Corrupted\", lambda d: (d, {\"name\": \"No Cleaning\", \"structural_drop\": 0, \"strategy_drop\": 0})),\n",
        "            (\"1_Basic\", lambda d: clean.clean_basic(d)),\n",
        "            (\"2_Heuristic\", lambda d: clean.clean_heuristic(d)),\n",
        "            (\"3_Semantic\", lambda d: clean.clean_semantic(d)),\n",
        "            (\"4_ModelAware\", lambda d: clean.clean_model_aware(d, build_model())),\n",
        "        ]\n",
        "\n",
        "        level_0_acc = None\n",
        "\n",
        "        for clean_idx, (clean_name, clean_func) in enumerate(cleaning_strategies):\n",
        "            print(f\"\\n  >>> {clean_name}\")\n",
        "            n_before = len(corrupted_df)\n",
        "            cleaning_time = 0 #default for level 0\n",
        "            if clean_idx > 0:\n",
        "                print(f\"Cleaning...\")\n",
        "                with timer() as t_clean:\n",
        "                    cleaned_df, meta = clean_func(corrupted_df.copy())\n",
        "                cleaning_time = t_clean()\n",
        "            else:\n",
        "                #for level 0, no cleaning applied\n",
        "                cleaned_df, meta = clean_func(corrupted_df.copy())\n",
        "\n",
        "                # drop rows where label is not numeric to avoid crash\n",
        "                cleaned_df['label'] = pd.to_numeric(cleaned_df['label'], errors='coerce')\n",
        "                initial_len = len(cleaned_df)\n",
        "                cleaned_df = cleaned_df.dropna(subset=['label'])\n",
        "                meta['structural_drop'] = initial_len - len(cleaned_df)\n",
        "            print(f\"Cleaning took: {cleaning_time:.2f} seconds\")\n",
        "\n",
        "            # cleaning stats\n",
        "            if meta.get(\"dropped\", 0) > 0:\n",
        "                print(f\"Dropped: {meta['dropped']} rows\")\n",
        "            if meta.get(\"modified\", 0) > 0:\n",
        "                print(f\"Modified: {meta['modified']} rows\")\n",
        "\n",
        "            print(f\"Current size: {len(cleaned_df):,} rows\")\n",
        "\n",
        "            # evaluate\n",
        "            print(f\"Training & evaluating...\")\n",
        "            with timer() as t_eval:\n",
        "                metrics, stats = train_and_evaluate(cleaned_df, cleaned_df, train_ids, test_ids)\n",
        "            eval_time = t_eval()\n",
        "            print(f\"Evaluation took: {eval_time:.2f} seconds\")\n",
        "\n",
        "            if metrics is None:\n",
        "                print(f\"Skipped (insufficient data)\")\n",
        "                continue\n",
        "\n",
        "            current_acc = metrics['accuracy']\n",
        "\n",
        "            # corrupted accuracy for recovery calculation\n",
        "            if clean_idx == 0:\n",
        "                level_0_acc = metrics['accuracy']\n",
        "                acc_delta = 0.0\n",
        "                acc_improv_pct = 0.0\n",
        "                recovery_pct = 0.0\n",
        "\n",
        "            # recovery computation\n",
        "            if clean_idx > 0 and level_0_acc is not None:\n",
        "                acc_drop = baseline_acc - level_0_acc\n",
        "                acc_delta = current_acc - level_0_acc\n",
        "                acc_improv_pct = (acc_delta / level_0_acc * 100) if level_0_acc > 0 else 0\n",
        "                acc_recovery = current_acc - level_0_acc\n",
        "                recovery_pct = (acc_recovery / acc_drop * 100) if acc_drop != 0 else 0\n",
        "            else:\n",
        "                recovery_pct = 0\n",
        "\n",
        "            print(f\"Acc: {current_acc:.4f} | F1: {metrics['f1']:.4f} | MAE: {metrics['mae']:.4f}\")\n",
        "            if clean_idx > 0:\n",
        "                print(f\"Accuracy Change vs Corrupt: {acc_delta:.4f} ({acc_improv_pct:.2f}%)\")\n",
        "                print(f\"Recovery from baseline: {recovery_pct:.1f}%\")\n",
        "\n",
        "            # 'stats' (from eval), 'metrics' (performance), and 'meta' (from cleaning functions)\n",
        "            result_row = {\n",
        "                \"batch\": batch_id,\n",
        "                \"experiment\": exp_id,\n",
        "                \"cleaning_name\": clean_name,\n",
        "                \"cleaning_num\": clean_idx,\n",
        "                **stats, **metrics,\n",
        "                \"baseline_acc\": baseline_acc,\n",
        "                \"level_0_acc\": level_0_acc or 0,\n",
        "                \"recovery_pct\": recovery_pct,\n",
        "                \"corrupt_time\": corrupt_time,\n",
        "                \"cleaning_time\": cleaning_time,\n",
        "                \"eval_time\": eval_time,\n",
        "                \"n_before\": n_before,\n",
        "                \"n_after\": len(cleaned_df),\n",
        "                **meta\n",
        "            }\n",
        "            batch_results.append(result_row)\n",
        "            if clean_idx == 0:\n",
        "                level_0_acc = current_acc\n",
        "\n",
        "        del corrupted_df\n",
        "        clear_memory()\n",
        "\n",
        "    # Save and return\n",
        "    batch_df = pd.DataFrame(batch_results)\n",
        "    batch_path = os.path.join(RESULTS_DIR, f\"{batch_id}_results.csv\")\n",
        "    batch_df.to_csv(batch_path, index=False)\n",
        "    print(f\"\\n{batch_id} complete! Results saved.\")\n",
        "    return batch_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by1T0Gfo8FCz",
        "outputId": "3485716a-f079-403d-9e2a-4d73c37c67e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------\n",
            "[1/4] 01_missing_text in batch1\n",
            "----------------------------------------------------------\n",
            "\n",
            "Corruption took: 0.14 seconds\n",
            "\n",
            "  >>> 0_Corrupted\n",
            "Cleaning took: 0.00 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 106.72 seconds\n",
            "Acc: 0.4177 | F1: 0.4021 | MAE: 1.0486\n",
            "\n",
            "  >>> 1_Basic\n",
            "Cleaning...\n",
            "Cleaning took: 9.12 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 97.75 seconds\n",
            "Acc: 0.4209 | F1: 0.4059 | MAE: 1.0458\n",
            "Accuracy Change vs Corrupt: 0.0032 (0.77%)\n",
            "Recovery from baseline: 3.3%\n",
            "\n",
            "\n",
            "  >>> 2_Heuristic\n",
            "Cleaning...\n",
            "Cleaning took: 5.50 seconds\n",
            "Current size: 68,669 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 77.12 seconds\n",
            "Acc: 0.5116 | F1: 0.5082 | MAE: 0.6399\n",
            "Accuracy Change vs Corrupt: 0.0940 (22.50%)\n",
            "Recovery from baseline: 96.3%\n",
            "\n",
            "\n",
            "  >>> 3_Semantic\n",
            "Cleaning...\n",
            "Cleaning took: 24.09 seconds\n",
            "Modified: 99331 rows\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 88.81 seconds\n",
            "Acc: 0.4189 | F1: 0.4103 | MAE: 1.0477\n",
            "Accuracy Change vs Corrupt: 0.0012 (0.29%)\n",
            "Recovery from baseline: 1.2%\n",
            "\n",
            "\n",
            "  >>> 4_ModelAware\n",
            "Cleaning...\n",
            "Cleaning took: 363.86 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 77.66 seconds\n",
            "Acc: 0.4365 | F1: 0.4204 | MAE: 1.0502\n",
            "Accuracy Change vs Corrupt: 0.0189 (4.53%)\n",
            "Recovery from baseline: 19.4%\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[2/4] 02_broken_chars in batch1\n",
            "----------------------------------------------------------\n",
            "\n",
            "Corruption took: 6.67 seconds\n",
            "\n",
            "  >>> 0_Corrupted\n",
            "Cleaning took: 0.00 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 119.18 seconds\n",
            "Acc: 0.5027 | F1: 0.4990 | MAE: 0.6716\n",
            "\n",
            "  >>> 1_Basic\n",
            "Cleaning...\n",
            "Cleaning took: 15.39 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 120.97 seconds\n",
            "Acc: 0.5075 | F1: 0.5033 | MAE: 0.6568\n",
            "Accuracy Change vs Corrupt: 0.0048 (0.96%)\n",
            "Recovery from baseline: 38.6%\n",
            "\n",
            "\n",
            "  >>> 2_Heuristic\n",
            "Cleaning...\n",
            "Cleaning took: 5.11 seconds\n",
            "Current size: 96,881 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 107.53 seconds\n",
            "Acc: 0.5057 | F1: 0.5018 | MAE: 0.6623\n",
            "Accuracy Change vs Corrupt: 0.0030 (0.59%)\n",
            "Recovery from baseline: 23.7%\n",
            "\n",
            "\n",
            "  >>> 3_Semantic\n",
            "Cleaning...\n",
            "Cleaning took: 24.15 seconds\n",
            "Modified: 99272 rows\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 107.59 seconds\n",
            "Acc: 0.4914 | F1: 0.4865 | MAE: 0.7062\n",
            "Accuracy Change vs Corrupt: -0.0113 (-2.25%)\n",
            "Recovery from baseline: -90.0%\n",
            "\n",
            "\n",
            "  >>> 4_ModelAware\n",
            "Cleaning...\n",
            "Cleaning took: 423.68 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 92.19 seconds\n",
            "Acc: 0.5329 | F1: 0.5245 | MAE: 0.6585\n",
            "Accuracy Change vs Corrupt: 0.0302 (6.00%)\n",
            "Recovery from baseline: 240.2%\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[3/4] 03_swapped_text in batch1\n",
            "----------------------------------------------------------\n",
            "\n",
            "Corruption took: 0.05 seconds\n",
            "\n",
            "  >>> 0_Corrupted\n",
            "Cleaning took: 0.00 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 131.23 seconds\n",
            "Acc: 0.4417 | F1: 0.4383 | MAE: 0.8615\n",
            "\n",
            "  >>> 1_Basic\n",
            "Cleaning...\n",
            "Cleaning took: 11.32 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 134.10 seconds\n",
            "Acc: 0.4446 | F1: 0.4405 | MAE: 0.8517\n",
            "Accuracy Change vs Corrupt: 0.0028 (0.63%)\n",
            "Recovery from baseline: 3.8%\n",
            "\n",
            "\n",
            "  >>> 2_Heuristic\n",
            "Cleaning...\n",
            "Cleaning took: 5.82 seconds\n",
            "Current size: 98,533 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 123.83 seconds\n",
            "Acc: 0.4438 | F1: 0.4402 | MAE: 0.8555\n",
            "Accuracy Change vs Corrupt: 0.0021 (0.47%)\n",
            "Recovery from baseline: 2.9%\n",
            "\n",
            "\n",
            "  >>> 3_Semantic\n",
            "Cleaning...\n",
            "Cleaning took: 23.88 seconds\n",
            "Modified: 99039 rows\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 111.86 seconds\n",
            "Acc: 0.4456 | F1: 0.4421 | MAE: 0.8436\n",
            "Accuracy Change vs Corrupt: 0.0039 (0.87%)\n",
            "Recovery from baseline: 5.2%\n",
            "\n",
            "\n",
            "  >>> 4_ModelAware\n",
            "Cleaning...\n",
            "Cleaning took: 466.82 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 97.71 seconds\n",
            "Acc: 0.4792 | F1: 0.4725 | MAE: 0.8185\n",
            "Accuracy Change vs Corrupt: 0.0374 (8.47%)\n",
            "Recovery from baseline: 50.9%\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[4/4] 04_missing_labels in batch1\n",
            "----------------------------------------------------------\n",
            "\n",
            "Corruption took: 0.06 seconds\n",
            "\n",
            "  >>> 0_Corrupted\n",
            "Cleaning took: 0.00 seconds\n",
            "Current size: 85,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 100.70 seconds\n",
            "Acc: 0.5152 | F1: 0.5113 | MAE: 0.6391\n",
            "\n",
            "  >>> 1_Basic\n",
            "Cleaning...\n",
            "Cleaning took: 10.26 seconds\n",
            "Current size: 85,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 104.21 seconds\n",
            "Acc: 0.5184 | F1: 0.5144 | MAE: 0.6296\n",
            "Accuracy Change vs Corrupt: 0.0032 (0.63%)\n",
            "Recovery from baseline: 4747.5%\n",
            "\n",
            "\n",
            "  >>> 2_Heuristic\n",
            "Cleaning...\n",
            "Cleaning took: 5.77 seconds\n",
            "Current size: 83,753 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 90.87 seconds\n",
            "Acc: 0.5161 | F1: 0.5122 | MAE: 0.6354\n",
            "Accuracy Change vs Corrupt: 0.0009 (0.18%)\n",
            "Recovery from baseline: 1347.0%\n",
            "\n",
            "\n",
            "  >>> 3_Semantic\n",
            "Cleaning...\n",
            "Cleaning took: 21.79 seconds\n",
            "Modified: 84189 rows\n",
            "Current size: 85,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 95.62 seconds\n",
            "Acc: 0.5159 | F1: 0.5122 | MAE: 0.6272\n",
            "Accuracy Change vs Corrupt: 0.0008 (0.15%)\n",
            "Recovery from baseline: 1122.1%\n",
            "\n",
            "\n",
            "  >>> 4_ModelAware\n",
            "Cleaning...\n",
            "Cleaning took: 369.85 seconds\n",
            "Dropped: 15000 rows\n",
            "Current size: 85,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 78.64 seconds\n",
            "Acc: 0.5393 | F1: 0.5318 | MAE: 0.6395\n",
            "Accuracy Change vs Corrupt: 0.0241 (4.68%)\n",
            "Recovery from baseline: 35390.6%\n",
            "\n",
            "\n",
            "batch1 complete! Results saved.\n"
          ]
        }
      ],
      "source": [
        "# Batch 1\n",
        "batch1_corruptions = [\n",
        "    (\"01_missing_text\", corrupt.apply_missing_text, {}),\n",
        "    (\"02_broken_chars\", corrupt.apply_broken_characters, {}),\n",
        "    (\"03_swapped_text\", corrupt.apply_swapped_text, {}),\n",
        "    (\"04_missing_labels\", corrupt.apply_missing_labels, {}),\n",
        "]\n",
        "\n",
        "batch1_df = run_batch_experiments(\"batch1\", batch1_corruptions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9svvMxf8FCz",
        "outputId": "8f2e2c72-6819-46bd-acf4-7bea5060425a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------\n",
            "[1/5] 05_swapped_labels in batch2\n",
            "----------------------------------------------------------\n",
            "\n",
            "Corruption took: 0.03 seconds\n",
            "\n",
            "  >>> 0_Corrupted\n",
            "Cleaning took: 0.00 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 118.40 seconds\n",
            "Acc: 0.4702 | F1: 0.4668 | MAE: 0.7694\n",
            "\n",
            "  >>> 1_Basic\n",
            "Cleaning...\n",
            "Cleaning took: 11.27 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 127.54 seconds\n",
            "Acc: 0.4763 | F1: 0.4729 | MAE: 0.7609\n",
            "Accuracy Change vs Corrupt: 0.0061 (1.31%)\n",
            "Recovery from baseline: 13.6%\n",
            "\n",
            "\n",
            "  >>> 2_Heuristic\n",
            "Cleaning...\n",
            "Cleaning took: 5.42 seconds\n",
            "Current size: 98,533 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 115.15 seconds\n",
            "Acc: 0.4722 | F1: 0.4688 | MAE: 0.7639\n",
            "Accuracy Change vs Corrupt: 0.0020 (0.43%)\n",
            "Recovery from baseline: 4.5%\n",
            "\n",
            "\n",
            "  >>> 3_Semantic\n",
            "Cleaning...\n",
            "Cleaning took: 24.80 seconds\n",
            "Modified: 99039 rows\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 102.92 seconds\n",
            "Acc: 0.4778 | F1: 0.4742 | MAE: 0.7550\n",
            "Accuracy Change vs Corrupt: 0.0076 (1.62%)\n",
            "Recovery from baseline: 16.9%\n",
            "\n",
            "\n",
            "  >>> 4_ModelAware\n",
            "Cleaning...\n",
            "Cleaning took: 441.87 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 99.16 seconds\n",
            "Acc: 0.5054 | F1: 0.4987 | MAE: 0.7420\n",
            "Accuracy Change vs Corrupt: 0.0352 (7.50%)\n",
            "Recovery from baseline: 78.2%\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[2/5] 06_combined_broken_chars_missing_text in batch2\n",
            "----------------------------------------------------------\n",
            "\n",
            "Corruption took: 4.61 seconds\n",
            "\n",
            "  >>> 0_Corrupted\n",
            "Cleaning took: 0.00 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 145.32 seconds\n",
            "Acc: 0.4798 | F1: 0.4806 | MAE: 0.7089\n",
            "\n",
            "  >>> 1_Basic\n",
            "Cleaning...\n",
            "Cleaning took: 12.70 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 124.66 seconds\n",
            "Acc: 0.4837 | F1: 0.4843 | MAE: 0.6990\n",
            "Accuracy Change vs Corrupt: 0.0040 (0.82%)\n",
            "Recovery from baseline: 11.1%\n",
            "\n",
            "\n",
            "  >>> 2_Heuristic\n",
            "Cleaning...\n",
            "Cleaning took: 4.83 seconds\n",
            "Current size: 89,976 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 110.59 seconds\n",
            "Acc: 0.5047 | F1: 0.5006 | MAE: 0.6621\n",
            "Accuracy Change vs Corrupt: 0.0250 (5.21%)\n",
            "Recovery from baseline: 70.4%\n",
            "\n",
            "\n",
            "  >>> 3_Semantic\n",
            "Cleaning...\n",
            "Cleaning took: 23.60 seconds\n",
            "Modified: 99215 rows\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 98.63 seconds\n",
            "Acc: 0.4735 | F1: 0.4739 | MAE: 0.7170\n",
            "Accuracy Change vs Corrupt: -0.0063 (-1.30%)\n",
            "Recovery from baseline: -17.6%\n",
            "\n",
            "\n",
            "  >>> 4_ModelAware\n",
            "Cleaning...\n",
            "Cleaning took: 420.57 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 103.24 seconds\n",
            "Acc: 0.5061 | F1: 0.5032 | MAE: 0.6996\n",
            "Accuracy Change vs Corrupt: 0.0263 (5.49%)\n",
            "Recovery from baseline: 74.2%\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[3/5] 07_combined_swap_text_labels in batch2\n",
            "----------------------------------------------------------\n",
            "\n",
            "Corruption took: 0.09 seconds\n",
            "\n",
            "  >>> 0_Corrupted\n",
            "Cleaning took: 0.00 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 138.21 seconds\n",
            "Acc: 0.4624 | F1: 0.4590 | MAE: 0.7962\n",
            "\n",
            "  >>> 1_Basic\n",
            "Cleaning...\n",
            "Cleaning took: 10.82 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 129.70 seconds\n",
            "Acc: 0.4668 | F1: 0.4636 | MAE: 0.7869\n",
            "Accuracy Change vs Corrupt: 0.0045 (0.96%)\n",
            "Recovery from baseline: 8.4%\n",
            "\n",
            "\n",
            "  >>> 2_Heuristic\n",
            "Cleaning...\n",
            "Cleaning took: 5.42 seconds\n",
            "Current size: 98,533 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 123.99 seconds\n",
            "Acc: 0.4627 | F1: 0.4594 | MAE: 0.7952\n",
            "Accuracy Change vs Corrupt: 0.0003 (0.06%)\n",
            "Recovery from baseline: 0.6%\n",
            "\n",
            "\n",
            "  >>> 3_Semantic\n",
            "Cleaning...\n",
            "Cleaning took: 25.11 seconds\n",
            "Modified: 99039 rows\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 104.60 seconds\n",
            "Acc: 0.4647 | F1: 0.4610 | MAE: 0.7884\n",
            "Accuracy Change vs Corrupt: 0.0023 (0.49%)\n",
            "Recovery from baseline: 4.3%\n",
            "\n",
            "\n",
            "  >>> 4_ModelAware\n",
            "Cleaning...\n",
            "Cleaning took: 438.18 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 96.44 seconds\n",
            "Acc: 0.4974 | F1: 0.4915 | MAE: 0.7621\n",
            "Accuracy Change vs Corrupt: 0.0350 (7.57%)\n",
            "Recovery from baseline: 66.2%\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[4/5] 08_heavy_missing in batch2\n",
            "----------------------------------------------------------\n",
            "\n",
            "Corruption took: 0.07 seconds\n",
            "\n",
            "  >>> 0_Corrupted\n",
            "Cleaning took: 0.00 seconds\n",
            "Current size: 90,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 88.15 seconds\n",
            "Acc: 0.4306 | F1: 0.4383 | MAE: 0.7944\n",
            "\n",
            "  >>> 1_Basic\n",
            "Cleaning...\n",
            "Cleaning took: 9.34 seconds\n",
            "Current size: 90,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 86.65 seconds\n",
            "Acc: 0.4335 | F1: 0.4420 | MAE: 0.7846\n",
            "Accuracy Change vs Corrupt: 0.0029 (0.68%)\n",
            "Recovery from baseline: 3.5%\n",
            "\n",
            "\n",
            "  >>> 2_Heuristic\n",
            "Cleaning...\n",
            "Cleaning took: 4.43 seconds\n",
            "Current size: 66,295 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 76.61 seconds\n",
            "Acc: 0.5098 | F1: 0.5062 | MAE: 0.6499\n",
            "Accuracy Change vs Corrupt: 0.0792 (18.39%)\n",
            "Recovery from baseline: 93.5%\n",
            "\n",
            "\n",
            "  >>> 3_Semantic\n",
            "Cleaning...\n",
            "Cleaning took: 17.33 seconds\n",
            "Modified: 89364 rows\n",
            "Current size: 90,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 80.07 seconds\n",
            "Acc: 0.4311 | F1: 0.4389 | MAE: 0.7834\n",
            "Accuracy Change vs Corrupt: 0.0005 (0.12%)\n",
            "Recovery from baseline: 0.6%\n",
            "\n",
            "\n",
            "  >>> 4_ModelAware\n",
            "Cleaning...\n",
            "Cleaning took: 324.75 seconds\n",
            "Dropped: 10000 rows\n",
            "Current size: 90,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 67.06 seconds\n",
            "Acc: 0.4496 | F1: 0.4551 | MAE: 0.7936\n",
            "Accuracy Change vs Corrupt: 0.0190 (4.41%)\n",
            "Recovery from baseline: 22.4%\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[5/5] 09_all_corruptions in batch2\n",
            "----------------------------------------------------------\n",
            "\n",
            "Corruption took: 4.34 seconds\n",
            "\n",
            "  >>> 0_Corrupted\n",
            "Cleaning took: 0.00 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 136.71 seconds\n",
            "Acc: 0.4613 | F1: 0.4542 | MAE: 0.8213\n",
            "\n",
            "  >>> 1_Basic\n",
            "Cleaning...\n",
            "Cleaning took: 12.16 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 130.93 seconds\n",
            "Acc: 0.4652 | F1: 0.4579 | MAE: 0.8145\n",
            "Accuracy Change vs Corrupt: 0.0039 (0.83%)\n",
            "Recovery from baseline: 7.1%\n",
            "\n",
            "\n",
            "  >>> 2_Heuristic\n",
            "Cleaning...\n",
            "Cleaning took: 4.67 seconds\n",
            "Current size: 93,060 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 109.75 seconds\n",
            "Acc: 0.4776 | F1: 0.4736 | MAE: 0.7525\n",
            "Accuracy Change vs Corrupt: 0.0163 (3.54%)\n",
            "Recovery from baseline: 30.3%\n",
            "\n",
            "\n",
            "  >>> 3_Semantic\n",
            "Cleaning...\n",
            "Cleaning took: 24.20 seconds\n",
            "Modified: 99170 rows\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 106.51 seconds\n",
            "Acc: 0.4595 | F1: 0.4519 | MAE: 0.8235\n",
            "Accuracy Change vs Corrupt: -0.0018 (-0.40%)\n",
            "Recovery from baseline: -3.4%\n",
            "\n",
            "\n",
            "  >>> 4_ModelAware\n",
            "Cleaning...\n",
            "Cleaning took: 452.90 seconds\n",
            "Current size: 100,000 rows\n",
            "Training & evaluating...\n",
            "Evaluation took: 91.44 seconds\n",
            "Acc: 0.4910 | F1: 0.4808 | MAE: 0.8044\n",
            "Accuracy Change vs Corrupt: 0.0297 (6.44%)\n",
            "Recovery from baseline: 55.1%\n",
            "\n",
            "\n",
            "batch2 complete! Results saved.\n"
          ]
        }
      ],
      "source": [
        "# Batch 2\n",
        "batch2_corruptions = [\n",
        "    (\"05_swapped_labels\", corrupt.apply_swapped_labels, {}),\n",
        "    (\"06_combined_broken_chars_missing_text\", corrupt.apply_combined_broken_chars_missing_text, {}),\n",
        "    (\"07_combined_swap_text_labels\", corrupt.apply_combined_swap_text_labels, {}),\n",
        "    (\"08_heavy_missing\", corrupt.apply_heavy_missing, {}),\n",
        "    (\"09_all_corruptions\", corrupt.apply_all_corruptions, {})\n",
        "]\n",
        "\n",
        "batch2_df = run_batch_experiments(\"batch2\", batch2_corruptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVuNmLPJWu14"
      },
      "source": [
        "### Combining batch1 and batch2 results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNuWEg5RWFfg",
        "outputId": "529e2291-e561-4d0c-8b16-0f96e788e612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined and saved results.\n",
            "Total evaluations: 45\n"
          ]
        }
      ],
      "source": [
        "batch1_path = os.path.join(RESULTS_DIR, \"batch1_results.csv\")\n",
        "batch1_df = pd.read_csv(batch1_path)\n",
        "batch2_path = os.path.join(RESULTS_DIR, \"batch2_results.csv\")\n",
        "batch2_df = pd.read_csv(batch2_path)\n",
        "all_results_df = pd.concat([batch1_df, batch2_df], ignore_index=True)\n",
        "combined_results_path = os.path.join(RESULTS_DIR, \"all_results_combined.csv\")\n",
        "all_results_df.to_csv(combined_results_path, index=False)\n",
        "\n",
        "print(f\"Combined and saved results.\")\n",
        "print(f\"Total evaluations: {len(all_results_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etYq3gKs14t5"
      },
      "source": [
        "## Analysis of Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDIs9wxQ8FCz",
        "outputId": "fb8c9d6f-785c-414f-819d-7035c82d7ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded results DF with 45 rows for analysis\n"
          ]
        }
      ],
      "source": [
        "results_df = pd.read_csv(combined_results_path)\n",
        "print(f\"Loaded results DF with {len(results_df)} rows for analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWIRTnWc8FCz"
      },
      "outputs": [],
      "source": [
        "# Constants for analysis\n",
        "PERFORMANCE_DIR = os.path.join(FIGURES_DIR, \"performance_analysis/\")\n",
        "os.makedirs(PERFORMANCE_DIR, exist_ok=True)\n",
        "\n",
        "COMP_DIR = os.path.join(FIGURES_DIR, \"computation_analysis/\")\n",
        "os.makedirs(COMP_DIR, exist_ok=True)\n",
        "\n",
        "CLEANLAB_DIR = os.path.join(FIGURES_DIR, \"cleanlab_analysis/\")\n",
        "os.makedirs(CLEANLAB_DIR, exist_ok=True)\n",
        "\n",
        "STATS_DIR = os.path.join(RESULTS_DIR, \"stats\")\n",
        "os.makedirs(STATS_DIR, exist_ok=True)\n",
        "\n",
        "STAT_TABLES_DIR = os.path.join(STATS_DIR, \"tables\")\n",
        "os.makedirs(STAT_TABLES_DIR, exist_ok=True)\n",
        "\n",
        "STAT_TESTS_DIR = os.path.join(STATS_DIR, \"statistical_tests\")\n",
        "os.makedirs(STAT_TESTS_DIR, exist_ok=True)\n",
        "\n",
        "BASELINE_ACC = results_df['baseline_acc'].iloc[0]\n",
        "CLEANING_STRATEGIES_ORDER = ['1_Basic', '2_Heuristic', '3_Semantic', '4_ModelAware']\n",
        "ALL_CLEANING_ORDER = ['0_Corrupted'] + CLEANING_STRATEGIES_ORDER\n",
        "HEATMAP_COLS = ['0_Corrupted'] + CLEANING_STRATEGIES_ORDER\n",
        "PLOT_STYLE = {\n",
        "    'figure.dpi': 300,\n",
        "    'savefig.dpi': 300,\n",
        "    'font.size': 10,\n",
        "    'axes.labelsize': 11,\n",
        "    'axes.titlesize': 12,\n",
        "    'xtick.labelsize': 9,\n",
        "    'ytick.labelsize': 9\n",
        "}\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams.update(PLOT_STYLE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJCrcqTw8FC0"
      },
      "outputs": [],
      "source": [
        "# Helper function for plot saving\n",
        "def save_plot(plot_type, plot_title, filename, **kwargs):\n",
        "    \"\"\"Helper to create, save, and close plots\"\"\"\n",
        "    print(f\"Generating plot: {plot_title}\")\n",
        "    plt.tight_layout()\n",
        "    full_path = os.path.join(PERFORMANCE_DIR if plot_type == \"performance\" else COMP_DIR if plot_type == \"computation\" else CLEANLAB_DIR, filename)\n",
        "    plt.savefig(full_path, **kwargs)\n",
        "    plt.close()\n",
        "    print(f\"Plot saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BVmFVK38FC0"
      },
      "source": [
        "### Corruption Impact Analysis on Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6azktVFpae7L",
        "outputId": "617daa8b-dccd-41c2-a694-26e7e539888c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CORRUPTION IMPACT ANALYSIS (Baseline vs. Corrupted)\n",
            "                           Experiment  Original_Acc  Corrupted_Acc  Abs_Damage  Damage_% Severity\n",
            "                      01_missing_text       0.51525       0.417650    0.097600 18.942261   Severe\n",
            "                     08_heavy_missing       0.51525       0.430601    0.084649 16.428718   Severe\n",
            "                      03_swapped_text       0.51525       0.441750    0.073500 14.264920 Moderate\n",
            "                   09_all_corruptions       0.51525       0.461300    0.053950 10.470645 Moderate\n",
            "         07_combined_swap_text_labels       0.51525       0.462400    0.052850 10.257157 Moderate\n",
            "                    05_swapped_labels       0.51525       0.470150    0.045100  8.753033 Moderate\n",
            "06_combined_broken_chars_missing_text       0.51525       0.479750    0.035500  6.889859 Moderate\n",
            "                      02_broken_chars       0.51525       0.502700    0.012550  2.435711  Minimal\n",
            "                    04_missing_labels       0.51525       0.515182    0.000068  0.013231  Minimal\n",
            "\n",
            "KEY FINDINGS: NOISE SENSITIVITY\n",
            "Most Destructive Corruption: 01_missing_text\n",
            "Accuracy dropped by 18.9% (Severe)\n",
            "\n",
            "Most Resilient Corruption:  02_broken_chars\n",
            "Accuracy dropped by 2.4% (Minimal)\n",
            "\n",
            "Average Accuracy Drop across all 4 types: 0.0506\n"
          ]
        }
      ],
      "source": [
        "df = results_df.copy()\n",
        "\n",
        "def interpret_damage(d):\n",
        "    abs_d = abs(d)\n",
        "    if abs_d < 0.03: return \"Minimal\"\n",
        "    elif abs_d < 0.08: return \"Moderate\"\n",
        "    elif abs_d < 0.15: return \"Severe\"\n",
        "    else: return \"Critical\"\n",
        "\n",
        "print(\"CORRUPTION IMPACT ANALYSIS (Baseline vs. Corrupted)\")\n",
        "\n",
        "corruption_df = df[df['cleaning_name'] == '0_Corrupted'].copy()\n",
        "\n",
        "corruption_df['Damage'] = corruption_df['baseline_acc'] - corruption_df['accuracy']\n",
        "corruption_df['Damage_Pct'] = (corruption_df['Damage'] / corruption_df['baseline_acc']) * 100\n",
        "corruption_df['Damage_Level'] = corruption_df['Damage'].apply(interpret_damage)\n",
        "\n",
        "report = corruption_df[[\n",
        "    'experiment', 'baseline_acc', 'accuracy', 'Damage', 'Damage_Pct', 'Damage_Level'\n",
        "]].copy()\n",
        "\n",
        "report.columns = ['Experiment', 'Original_Acc', 'Corrupted_Acc', 'Abs_Damage', 'Damage_%', 'Severity']\n",
        "\n",
        "# sort by damage\n",
        "report = report.sort_values(by='Abs_Damage', ascending=False)\n",
        "\n",
        "print(report.to_string(index=False))\n",
        "\n",
        "# Key Findings\n",
        "print(\"\\nKEY FINDINGS: NOISE SENSITIVITY\")\n",
        "\n",
        "# exclude missing_labels because that causes a crash\n",
        "most_dmg = report[report['Experiment'] != '04_missing_labels'].iloc[0]\n",
        "least_dmg = report[report['Experiment'] != '04_missing_labels'].iloc[-1]\n",
        "\n",
        "print(f\"Most Destructive Corruption: {most_dmg['Experiment']}\")\n",
        "print(f\"Accuracy dropped by {most_dmg['Damage_%']:.1f}% ({most_dmg['Severity']})\")\n",
        "\n",
        "print(f\"\\nMost Resilient Corruption:  {least_dmg['Experiment']}\")\n",
        "print(f\"Accuracy dropped by {least_dmg['Damage_%']:.1f}% ({least_dmg['Severity']})\")\n",
        "\n",
        "avg_dmg = report['Abs_Damage'].mean()\n",
        "print(f\"\\nAverage Accuracy Drop across all 4 types: {avg_dmg:.4f}\")\n",
        "\n",
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfY6-d9JwyPE"
      },
      "source": [
        "### Statistical Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analysis_df = results_df.copy()\n",
        "analysis_df = analysis_df[analysis_df[\"cleaning_name\"].isin(ALL_CLEANING_ORDER)].copy()\n",
        "\n",
        "# TEST 1: Statistical improvement vs corrupted baseline\n",
        "# check whether each cleaning method significantly improves accuracy compared to corrupted data\n",
        "stats_results = []\n",
        "\n",
        "for exp in analysis_df[\"experiment\"].unique():\n",
        "    exp_data = analysis_df[analysis_df[\"experiment\"] == exp]\n",
        "\n",
        "    corrupted = exp_data[exp_data[\"cleaning_name\"] == \"0_Corrupted\"]\n",
        "    if corrupted.empty:\n",
        "        continue\n",
        "\n",
        "    acc_corrupt = corrupted[\"accuracy\"].values[0]\n",
        "\n",
        "    for strategy in CLEANING_STRATEGIES_ORDER:\n",
        "        method = exp_data[exp_data[\"cleaning_name\"] == strategy]\n",
        "        if method.empty or strategy == \"0_Corrupted\":\n",
        "            continue\n",
        "\n",
        "        acc_clean = method[\"accuracy\"].values[0]\n",
        "        n_test = method[\"test_valid\"].values[0]\n",
        "\n",
        "        abs_gain = acc_clean - acc_corrupt\n",
        "        recovery_pct = method[\"recovery_pct\"].values[0]\n",
        "\n",
        "        se = np.sqrt(\n",
        "            (acc_corrupt * (1 - acc_corrupt) +\n",
        "             acc_clean * (1 - acc_clean)) / n_test\n",
        "        )\n",
        "\n",
        "        p_value = 2 * (1 - norm.cdf(abs(abs_gain / se))) if se > 0 else 1.0\n",
        "        sig = \"***\" if p_value < 0.001 else (\"**\" if p_value < 0.01 else (\"*\" if p_value < 0.05 else \"ns\"))\n",
        "\n",
        "        stats_results.append({\n",
        "            \"experiment\": exp,\n",
        "            \"cleaning_method\": strategy,\n",
        "            \"acc_corrupted\": acc_corrupt,\n",
        "            \"acc_cleaned\": acc_clean,\n",
        "            \"abs_gain\": abs_gain,\n",
        "            \"recovery_pct\": recovery_pct,\n",
        "            \"p_value\": p_value,\n",
        "            \"significance\": sig\n",
        "        })\n",
        "\n",
        "df_sig = pd.DataFrame(stats_results)\n",
        "df_sig.to_csv(os.path.join(STAT_TESTS_DIR, \"test1_significance_vs_corrupted.csv\"), index=False)\n",
        "\n",
        "# TEST 2: Best method per corruption\n",
        "# identify which cleaning method most frequently achieves the highest accuracy\n",
        "best = (\n",
        "    analysis_df.sort_values([\"experiment\", \"accuracy\"], ascending=[True, False])\n",
        "      .groupby(\"experiment\", as_index=False)\n",
        "      .first()\n",
        "      [[\"experiment\", \"cleaning_name\", \"accuracy\", \"f1\", \"mae\"]]\n",
        "      .rename(columns={\"cleaning_name\": \"best_method\", \"accuracy\": \"best_accuracy\"})\n",
        ")\n",
        "\n",
        "win_counts = best[\"best_method\"].value_counts().reset_index()\n",
        "win_counts.columns = [\"cleaning_method\", \"wins\"]\n",
        "best.to_csv(os.path.join(STAT_TESTS_DIR, \"test2_best_method_per_experiment.csv\"), index=False)\n",
        "win_counts.to_csv(os.path.join(STAT_TESTS_DIR, \"test2_win_counts.csv\"), index=False)\n",
        "\n",
        "# TEST 3: Pairwise improvement (ModelAware vs corrupted and vs heuristic) \n",
        "# Quantify magnitude of gains of the best method vs the corrupted and the 2nd best method\n",
        "wide_acc = analysis_df.pivot(index=\"experiment\", columns=\"cleaning_name\", values=\"accuracy\")\n",
        "\n",
        "impr = pd.DataFrame({\n",
        "    \"experiment\": wide_acc.index,\n",
        "    \"acc_modelaware_vs_corrupted\": wide_acc[\"4_ModelAware\"] - wide_acc[\"0_Corrupted\"],\n",
        "    \"acc_modelaware_vs_heuristic\": wide_acc[\"4_ModelAware\"] - wide_acc[\"2_Heuristic\"],\n",
        "}).reset_index(drop=True)\n",
        "impr.to_csv(os.path.join(STAT_TESTS_DIR, \"test3_accuracy_deltas.csv\"), index=False)\n",
        "\n",
        "# TEST 4: Metric consistency (accuracy, F1, MAE)\n",
        "# ensuring improvements are not only for one metric\n",
        "wide_f1  = analysis_df.pivot(index=\"experiment\", columns=\"cleaning_name\", values=\"f1\")\n",
        "wide_mae = analysis_df.pivot(index=\"experiment\", columns=\"cleaning_name\", values=\"mae\")\n",
        "\n",
        "consistency = pd.DataFrame({\n",
        "    \"experiment\": wide_acc.index,\n",
        "    \"acc_improved\": (wide_acc[\"4_ModelAware\"] > wide_acc[\"0_Corrupted\"]).astype(int),\n",
        "    \"f1_improved\": (wide_f1[\"4_ModelAware\"] > wide_f1[\"0_Corrupted\"]).astype(int),\n",
        "    \"mae_improved\": (wide_mae[\"4_ModelAware\"] < wide_mae[\"0_Corrupted\"]).astype(int),\n",
        "})\n",
        "\n",
        "consistency[\"n_metrics_improved\"] = consistency[[\"acc_improved\", \"f1_improved\", \"mae_improved\"]].sum(axis=1)\n",
        "consistency.to_csv(os.path.join(STAT_TESTS_DIR, \"test4_metric_consistency.csv\"), index=False)\n",
        "\n",
        "# TEST 5: Cost vs benefit (accuracy gain per second)\n",
        "# evaluate efficiency of cleaning\n",
        "t = analysis_df[analysis_df[\"cleaning_name\"].isin([\"0_Corrupted\", \"4_ModelAware\"])]\n",
        "w = t.pivot(index=\"experiment\", columns=\"cleaning_name\")\n",
        "\n",
        "cost = pd.DataFrame({\n",
        "    \"experiment\": w.index,\n",
        "    \"acc_gain\": w[\"accuracy\"][\"4_ModelAware\"] - w[\"accuracy\"][\"0_Corrupted\"],\n",
        "    \"clean_time\": w[\"cleaning_time\"][\"4_ModelAware\"],\n",
        "    \"eval_time\": w[\"eval_time\"][\"4_ModelAware\"],\n",
        "})\n",
        "\n",
        "cost[\"total_time\"] = cost[\"clean_time\"] + cost[\"eval_time\"]\n",
        "cost[\"gain_per_clean_sec\"] = cost[\"acc_gain\"] / cost[\"clean_time\"].replace(0, np.nan)\n",
        "cost[\"gain_per_total_sec\"] = cost[\"acc_gain\"] / cost[\"total_time\"].replace(0, np.nan)\n",
        "cost.to_csv(os.path.join(STAT_TESTS_DIR, \"test5_cost_benefit.csv\"), index=False)\n",
        "\n",
        "del analysis_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg9Jin-TPgX6"
      },
      "source": [
        "### Visual Analysis of Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNsZKvvgc7DD",
        "outputId": "808f0a90-5b83-408a-d61c-e40263be60e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating plot: Corruption Damage Ranking\n",
            "Plot saved.\n",
            "Generating plot: Average Model Accuracy\n",
            "Plot saved.\n",
            "Generating plot: Strategy Comparison Heatmap\n",
            "Plot saved.\n",
            "Generating plot: Best Strategy Performance Ranking\n",
            "Plot saved.\n",
            "Generating plot: Trade-off Data Loss vs Cleaning Effectiveness\n",
            "Plot saved.\n",
            "Generating plot: Accuracy Gain Heatmap\n",
            "Plot saved.\n",
            "Generating plot: Recovery Robustness\n",
            "Plot saved.\n",
            "Generating plot: Accuracy Distribution\n",
            "Plot saved.\n",
            "Generating plot: Impact of Corruption\n",
            "Plot saved.\n",
            "Generating plot: Performance Recovery Trends\n",
            "Plot saved.\n",
            "All performance analysis figures saved.\n"
          ]
        }
      ],
      "source": [
        "PERFORMANCE_DIR = os.path.join(FIGURES_DIR, \"performance_analysis\")\n",
        "os.makedirs(PERFORMANCE_DIR, exist_ok=True)\n",
        "\n",
        "# styles\n",
        "plt.rcParams.update(PLOT_STYLE)\n",
        "\n",
        "analysis_df = results_df.copy()\n",
        "\n",
        "analysis_df['data_loss_pct'] = ((analysis_df['n_before'] - analysis_df['n_after']) / analysis_df['n_before']) * 100\n",
        "analysis_df['accuracy_gain'] = analysis_df['accuracy'] - analysis_df['level_0_acc']\n",
        "\n",
        "# FIGURE 1: Corruption Damage Ranking (Bar Chart)\n",
        "corruption_data = analysis_df[analysis_df['cleaning_name'] == '0_Corrupted'].copy()\n",
        "corruption_data['Damage'] = corruption_data['baseline_acc'] - corruption_data['accuracy']\n",
        "corruption_data = corruption_data.sort_values('accuracy', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.barh(corruption_data['experiment'], corruption_data['Damage'],\n",
        "        color='salmon', edgecolor='black', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Accuracy Drop (Baseline - Corrupted)')\n",
        "ax.set_title('Corruption Impact: Accuracy Degradation by Type')\n",
        "ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "save_plot(\"performance\", \"Corruption Damage Ranking\", \"01_corruption_damage_ranking.png\")\n",
        "\n",
        "#FIGURE 2: Average Model Accuracy in increasing order (Bar Chart)\n",
        "avg_acc = analysis_df[analysis_df['cleaning_name'] != '0_Corrupted'].groupby('cleaning_name')['accuracy'].mean().reset_index()\n",
        "avg_corruption_acc = analysis_df[analysis_df['cleaning_name'] == '0_Corrupted']['accuracy'].mean()\n",
        "avg_acc = avg_acc.sort_values('accuracy', ascending=True)\n",
        "\n",
        "colors = plt.cm.viridis(np.linspace(0, 0.8, len(avg_acc)))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "bars = ax.bar(avg_acc['cleaning_name'], avg_acc['accuracy'],\n",
        "              color=colors, edgecolor='black', alpha=0.85)\n",
        "ax.axhline(y=avg_corruption_acc, color='red', linestyle='--', linewidth=2.5,\n",
        "           label=f'Avg Corrupted Acc ({avg_corruption_acc:.3f})', zorder=3)\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "            f'{height:.3f}', ha='center', va='bottom',\n",
        "            fontsize=10, fontweight='bold', color='black')\n",
        "\n",
        "ax.set_ylim(0.40, 0.52)\n",
        "import matplotlib.ticker as ticker\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.01))\n",
        "ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.0025))\n",
        "\n",
        "ax.grid(which='major', axis='y', linestyle='-', alpha=0.3, color='gray')\n",
        "ax.grid(which='minor', axis='y', linestyle=':', alpha=0.2, color='gray')\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "ax.set_xlabel('Cleaning Strategy', fontsize=12, labelpad=10)\n",
        "ax.set_ylabel('Average Model Accuracy', fontsize=12, labelpad=10)\n",
        "ax.set_title('Detailed Strategy Effectiveness vs. Baseline', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "ax.legend(frameon=True, shadow=True, loc='upper left')\n",
        "save_plot(\"performance\", \"Average Model Accuracy\", \"02_average_model_accuracy.png\")\n",
        "\n",
        "# FIGURE 3: Strategy Comparison Heatmap\n",
        "df_heatmap = analysis_df.pivot(index='experiment', columns='cleaning_name', values='accuracy')\n",
        "df_heatmap = df_heatmap[HEATMAP_COLS]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(df_heatmap, annot=True, fmt='.3f', cmap='YlGn',\n",
        "            linewidths=0.5, linecolor='white', ax=ax)\n",
        "ax.set_title('Strategy Comparison: Accuracy Across Cleaning Methods')\n",
        "ax.set_xlabel('Cleaning Strategy')\n",
        "ax.set_ylabel('Corruption Type')\n",
        "save_plot(\"performance\", \"Strategy Comparison Heatmap\", \"03_strategy_comparison_heatmap.png\")\n",
        "\n",
        "# FIGURE 4: Best Strategy Performance Ranking\n",
        "best_per_exp = analysis_df.loc[analysis_df.groupby('experiment')['accuracy'].idxmax()].copy()\n",
        "best_per_exp = best_per_exp.sort_values('accuracy', ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "unique_strats = best_per_exp['cleaning_name'].unique()\n",
        "colors = plt.get_cmap('Set2')(range(len(unique_strats)))\n",
        "strat_color_map = {strat: colors[i] for i, strat in enumerate(unique_strats)}\n",
        "\n",
        "ax.barh(best_per_exp['experiment'], best_per_exp['accuracy'],\n",
        "        color=[strat_color_map[s] for s in best_per_exp['cleaning_name']],\n",
        "        edgecolor='black', alpha=0.8)\n",
        "\n",
        "ax.axvline(x=BASELINE_ACC, color='blue', linestyle='--', linewidth=2, label=f'Baseline ({BASELINE_ACC:.2f})')\n",
        "ax.set_xlabel('Highest Accuracy Achieved')\n",
        "ax.set_title('Final Model Performance: Best Result per Corruption')\n",
        "ax.set_xlim(analysis_df['accuracy'].min() * 0.9, BASELINE_ACC * 1.1)\n",
        "\n",
        "legend_elements = [Patch(facecolor=strat_color_map[s], label=s) for s in unique_strats]\n",
        "legend_elements.append(plt.Line2D([0], [0], color='blue', linestyle='--', label='Baseline'))\n",
        "ax.legend(handles=legend_elements, loc='lower right')\n",
        "\n",
        "save_plot(\"performance\", \"Best Strategy Performance Ranking\", \"04_best_strategy_performance_ranking.png\")\n",
        "\n",
        "# FIGURE 5: Trade-off: Data Loss vs. Cleaning Effectiveness\n",
        "plt.figure(figsize=(10, 7))\n",
        "df_plot = analysis_df[analysis_df['cleaning_name'] != '0_Corrupted']\n",
        "sns.scatterplot(data=df_plot, x='data_loss_pct', y=\"accuracy\", hue='cleaning_name', style='cleaning_name', s=100)\n",
        "\n",
        "plt.axhline(y=BASELINE_ACC, color='blue', linestyle='--', alpha=0.5, label='Baseline')\n",
        "plt.ylabel('Model Accuracy')\n",
        "\n",
        "plt.title(\"Trade-off: Data Loss vs. Cleaning Effectiveness\", fontweight='bold')\n",
        "plt.xlabel('Data Loss (%)')\n",
        "plt.legend(title='Strategy', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "save_plot(\"performance\", \"Trade-off Data Loss vs Cleaning Effectiveness\", \"05_data_loss_vs_effectiveness.png\")\n",
        "\n",
        "# FIGURE 6: Accuracy Gain Heatmap\n",
        "pivot_acc = analysis_df.pivot(index='experiment', columns='cleaning_name', values='accuracy')[HEATMAP_COLS]\n",
        "delta_df = (pivot_acc.subtract(pivot_acc['0_Corrupted'], axis=0).divide(pivot_acc['0_Corrupted'], axis=0) * 100).drop(columns='0_Corrupted')\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(delta_df, annot=True, fmt='.3f', cmap='YlGn', linewidths=0.5, linecolor='white',\n",
        "            cbar_kws={'label': 'Absolute Accuracy Gain (Points)'})\n",
        "plt.title('Strategy Effectiveness: Accuracy Gain Over Corrupted Baseline', fontsize=14, fontweight='bold', pad=20)\n",
        "save_plot(\"performance\", \"Accuracy Gain Heatmap\", \"06_accuracy_gain_heatmap.png\")\n",
        "\n",
        "# FIGURE 7: Recovery Robustness\n",
        "df_strategies = analysis_df[analysis_df['cleaning_name'].isin(CLEANING_STRATEGIES_ORDER)].copy()\n",
        "df_strategies['cleaning_name'] = pd.Categorical(df_strategies['cleaning_name'], categories=CLEANING_STRATEGIES_ORDER, ordered=True)\n",
        "\n",
        "df_strategies['recovery_pct_viz'] = df_strategies['recovery_pct'].clip(lower=-20, upper=150)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='cleaning_name', y='recovery_pct_viz', data=df_strategies, palette='viridis', showmeans=True,\n",
        "            meanprops={\"marker\":\"s\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\"})\n",
        "sns.stripplot(x='cleaning_name', y='recovery_pct_viz', data=df_strategies, color=\".3\", alpha=0.6, size=6)\n",
        "plt.axhline(y=100, color='red', linestyle='--', label='100% Recovery')\n",
        "plt.title('Strategy Recovery Success (Capped at 150%)', fontweight='bold')\n",
        "plt.legend()\n",
        "save_plot(\"performance\", \"Recovery Robustness\", \"07_recovery_robustness_boxplot.png\")\n",
        "\n",
        "# FIGURE 8: Absolute Accuracy Boxplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "all_order = ['0_Corrupted'] + CLEANING_STRATEGIES_ORDER\n",
        "plot_all_df = analysis_df[analysis_df['cleaning_name'].isin(all_order)].copy()\n",
        "plot_all_df['cleaning_name'] = pd.Categorical(plot_all_df['cleaning_name'], categories=all_order, ordered=True)\n",
        "\n",
        "sns.boxplot(x='cleaning_name', y='accuracy', data=plot_all_df, palette='Set2', showmeans=True)\n",
        "sns.swarmplot(x='cleaning_name', y='accuracy', data=plot_all_df, color=\".25\", alpha=0.6)\n",
        "plt.axhline(y=BASELINE_ACC, color='blue', linestyle='--', linewidth=2, label=f'Baseline ({BASELINE_ACC:.3f})')\n",
        "plt.title('Performance Distribution: Absolute Accuracy', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "save_plot(\"performance\", \"Accuracy Distribution\", \"08_accuracy_distribution_boxplot.png\")\n",
        "\n",
        "# FIGURE 9: Impact Bar Chart\n",
        "df_corrupted = analysis_df[analysis_df['cleaning_name'] == '0_Corrupted'].copy()\n",
        "df_corrupted['Drop'] = df_corrupted['baseline_acc'] - df_corrupted['accuracy']\n",
        "df_corrupted = df_corrupted.sort_values('Drop', ascending=False)\n",
        "\n",
        "melted_df = df_corrupted.melt(id_vars=['experiment', 'Drop'], value_vars=['baseline_acc', 'accuracy'],\n",
        "                              var_name='Metrics', value_name='Accuracy')\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.barplot(data=melted_df, x='experiment', y='Accuracy', hue='Metrics', palette=['#3498db', '#e74c3c'])\n",
        "for i, (_, row) in enumerate(df_corrupted.iterrows()):\n",
        "    ax.text(i + 0.2, row['accuracy'] + 0.01, f\"-{row['Drop']:.1%}\",\n",
        "            ha='center', va='bottom', color='red', fontweight='bold', fontsize=9)\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Impact of Corruption: Baseline vs. Corrupted', fontweight='bold')\n",
        "save_plot(\"performance\", \"Impact of Corruption\", \"09_corruption_impact_bar_chart.png\")\n",
        "\n",
        "# FIGURE 10: Recovery Trends\n",
        "severity_order = df_corrupted.sort_values('Drop', ascending=False)['experiment'].unique().tolist()\n",
        "\n",
        "df_strat_trend = analysis_df[analysis_df['cleaning_name'].isin(CLEANING_STRATEGIES_ORDER)].copy()\n",
        "df_strat_trend['cleaning_name'] = pd.Categorical(df_strat_trend['cleaning_name'], categories=CLEANING_STRATEGIES_ORDER, ordered=True)\n",
        "df_strat_trend['experiment'] = pd.Categorical(df_strat_trend['experiment'], categories=severity_order, ordered=True)\n",
        "df_strat_trend['recovery_percent'] = df_strat_trend['recovery_pct'].clip(lower=-20, upper=120)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=df_strat_trend, x='experiment', y='recovery_percent', hue='cleaning_name',\n",
        "             marker='o', markersize=8, linewidth=2, palette='viridis')\n",
        "\n",
        "plt.axhline(y=100, color='red', linestyle='--', alpha=0.6, label='Full Recovery (100%)')\n",
        "plt.ylim(-30, 130)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Performance Recovery Trends Across Corruption Severity', fontweight='bold')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "save_plot(\"performance\", \"Performance Recovery Trends\", \"10_performance_recovery_trends.png\")\n",
        "\n",
        "print(f\"All performance analysis figures saved.\")\n",
        "\n",
        "del analysis_df, corruption_data, avg_acc, df_heatmap, best_per_exp, df_plot, delta_df, df_strategies, plot_all_df, df_corrupted, melted_df, df_strat_trend\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMpHEP_1PbT4"
      },
      "source": [
        "### Visual Analysis of Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFJw9yYND24q",
        "outputId": "7c254b93-a33e-4526-f688-cf2d0e018609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating plot: Time Breakdown Stacked Bar Chart\n",
            "Plot saved.\n",
            "Generating plot: Total Pipeline Time Comparison\n",
            "Plot saved.\n",
            "Generating plot: Accuracy vs Time Trade-off\n",
            "Plot saved.\n",
            "Generating plot: Cleaning Overhead Ratios\n",
            "Plot saved.\n",
            "Generating plot: Time Sensitivity per Corruption\n",
            "Plot saved.\n",
            "\n",
            "All computation analysis figures saved.\n"
          ]
        }
      ],
      "source": [
        "from matplotlib.ticker import ScalarFormatter\n",
        "\n",
        "\n",
        "COMP_DIR = os.path.join(FIGURES_DIR, \"computation_analysis\")\n",
        "os.makedirs(COMP_DIR, exist_ok=True)\n",
        "\n",
        "analysis_df = results_df.copy()\n",
        "\n",
        "# extra metrics\n",
        "analysis_df['total_time'] = analysis_df['corrupt_time'] + analysis_df['cleaning_time'] + analysis_df['eval_time']\n",
        "analysis_df['cleaning_ratio'] = analysis_df['cleaning_time'] / analysis_df['total_time']\n",
        "analysis_df['eval_ratio'] = analysis_df['eval_time'] / analysis_df['total_time']\n",
        "analysis_df['corrupt_ratio'] = analysis_df['corrupt_time'] / analysis_df['total_time']\n",
        "\n",
        "analysis_df['cleaning_name'] = pd.Categorical(analysis_df['cleaning_name'], categories=ALL_CLEANING_ORDER, ordered=True)\n",
        "\n",
        "# FIGURE 1: Time Breakdown Stacked Bar Chart\n",
        "avg_time = analysis_df.groupby('cleaning_name')[['cleaning_time', 'eval_time']].mean()\n",
        "\n",
        "ax = avg_time.plot(kind='bar', stacked=True, figsize=(10, 6), color=['#3498db', '#2ecc71'])\n",
        "plt.title('Average Computational Time Breakdown per Strategy', fontweight='bold')\n",
        "plt.ylabel('Time (Seconds)')\n",
        "plt.xlabel('Cleaning Strategy')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(['Cleaning Time', 'Eval Time'])\n",
        "save_plot(\"computation\", \"Time Breakdown Stacked Bar Chart\", \"01_time_breakdown_stacked_bar.png\")\n",
        "\n",
        "# FIGURE 2: Total Pipeline Time Comparision\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='cleaning_name', y='total_time', data=analysis_df, palette='magma', errorbar=None)\n",
        "plt.title('Total Pipeline Runtime Comparison (End-to-End)', fontweight='bold')\n",
        "plt.ylabel('Total Time (Seconds)')\n",
        "plt.xlabel('Cleaning Strategy')\n",
        "save_plot(\"computation\", \"Total Pipeline Time Comparison\", \"02_total_pipeline_time.png\")\n",
        "\n",
        "# FIGURE 3: Boxplot of Time Variability\n",
        "df_summary = analysis_df.groupby('cleaning_name').agg({\n",
        "    'accuracy': 'mean',\n",
        "    'total_time': 'mean'\n",
        "}).reset_index()\n",
        "df_summary = df_summary[df_summary['cleaning_name'] != '0_Corrupted'].copy()\n",
        "df_summary['label'] = df_summary['cleaning_name']\n",
        "\n",
        "df_summary = df_summary.sort_values('total_time')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set_style({'axes.grid': True, 'grid.linestyle': '--'})\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.plot(df_summary['total_time'], df_summary['accuracy'],\n",
        "         color='lightgrey', linestyle='--', linewidth=1.5, zorder=1)\n",
        "colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3']\n",
        "for i, (idx, row) in enumerate(df_summary.iterrows()):\n",
        "    plt.scatter(row['total_time'], row['accuracy'], s=500,\n",
        "                color=colors[i % len(colors)], edgecolor='black',\n",
        "                alpha=0.9, linewidth=1.5, zorder=2)\n",
        "\n",
        "    x_offset = 1.05\n",
        "    y_offset = 0.0008\n",
        "\n",
        "    plt.text(row['total_time'] * x_offset, row['accuracy'] + y_offset,\n",
        "             row['label'], fontweight='bold', fontsize=11,\n",
        "             verticalalignment='center')\n",
        "\n",
        "plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
        "plt.xticks([100, 150, 200, 300, 400, 500])\n",
        "\n",
        "plt.title('Accuracy vs. Time Trade-off', fontsize=16, fontweight='bold', pad=25)\n",
        "plt.xlabel('Total Computational Time (Seconds)', fontsize=12)\n",
        "plt.ylabel('Average Model Accuracy', fontsize=12)\n",
        "sns.despine(left=True, bottom=True)\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.4)\n",
        "save_plot(\"computation\", \"Accuracy vs Time Trade-off\", \"03_accuracy_vs_time_tradeoff.png\")\n",
        "\n",
        "# FIGURE 4: Cleaning Overhead Ratio\n",
        "ratio_data = analysis_df.groupby('cleaning_name')[['corrupt_ratio', 'cleaning_ratio', 'eval_ratio']].mean()\n",
        "\n",
        "ax = ratio_data.plot(kind='bar', stacked=True, figsize=(10, 6), color=['#95a5a6', '#3498db', '#2ecc71'])\n",
        "plt.title('Computational Overhead Ratio: Where is the bottleneck?', fontweight='bold')\n",
        "plt.ylabel('Percentage of Total Runtime')\n",
        "plt.xlabel('Cleaning Strategy')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(['Corrupt %', 'Cleaning %', 'Eval %'], loc='lower right')\n",
        "\n",
        "vals = ax.get_yticks()\n",
        "ax.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\n",
        "\n",
        "save_plot(\"computation\", \"Cleaning Overhead Ratios\", \"04_cleaning_overhead_ratios.png\")\n",
        "\n",
        "# FIGURE 5: Corruption-specific Time Sensitivity\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(\n",
        "    data=analysis_df,\n",
        "    x='experiment',\n",
        "    y='cleaning_time',\n",
        "    hue='cleaning_name',\n",
        "    marker='o',\n",
        "    markersize=8,\n",
        "    linewidth=2,\n",
        "    sort=False\n",
        ")\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.title(\n",
        "    'Cleaning Time Sensitivity across Corruption Types (Log Scale)',\n",
        "    fontweight='bold'\n",
        ")\n",
        "plt.ylabel('Cleaning Time (seconds, log scale)')\n",
        "plt.xlabel('Corruption Experiment')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.yticks([10, 100])\n",
        "plt.ylim(0.8, analysis_df['cleaning_time'].max() * 1.1)\n",
        "\n",
        "plt.legend(title='Strategy', loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot(\"computation\", \"Time Sensitivity per Corruption\", \"05_time_sensitivity_per_corruption.png\")\n",
        "\n",
        "print(\"\\nAll computation analysis figures saved.\")\n",
        "\n",
        "del analysis_df, avg_time, ratio_data, df_summary\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHc4sh0MPVmU"
      },
      "source": [
        "### Visual Analysis of Model Aware Reweighting using CleanLab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVPgSaow8FC1"
      },
      "source": [
        "#### Combining batch1 and batch2 cleanlab details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFaqnfT-L25O",
        "outputId": "3f8feeea-f695-4897-a103-52639df84d5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined and saved Cleanlab details.\n",
            "Total cleanlab evaluations: 9\n"
          ]
        }
      ],
      "source": [
        "batch1_cleanlab_path = os.path.join(RESULTS_DIR, \"batch1_cleanlab_details.csv\")\n",
        "batch2_cleanlab_path = os.path.join(RESULTS_DIR, \"batch2_cleanlab_details.csv\")\n",
        "\n",
        "df_cleanlab1 = pd.read_csv(batch1_cleanlab_path)\n",
        "df_cleanlab2 = pd.read_csv(batch2_cleanlab_path)\n",
        "\n",
        "df_cleanlab_combined = pd.concat([df_cleanlab1, df_cleanlab2], ignore_index=True)\n",
        "\n",
        "combined_cleanlab_path = os.path.join(RESULTS_DIR, \"all_cleanlab_stats_combined.csv\")\n",
        "\n",
        "df_cleanlab_combined.to_csv(combined_cleanlab_path, index=False)\n",
        "\n",
        "print(f\"Combined and saved Cleanlab details.\")\n",
        "print(f\"Total cleanlab evaluations: {len(df_cleanlab_combined)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkcNsyuY8FC1"
      },
      "source": [
        "#### Analysis and Visualization of CleanLab Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqTwQ9a7LdyJ",
        "outputId": "d404ca74-bf02-4471-8bc8-a7821c3690d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating plot: Confidence Separation\n",
            "Plot saved.\n",
            "Generating plot: Class-wise Noise Heatmap\n",
            "Plot saved.\n",
            "Generating plot: Reweighting Behavior Analysis\n",
            "Plot saved.\n",
            "All CleanLab analysis figures saved.\n"
          ]
        }
      ],
      "source": [
        "CLEANLAB_DIR = os.path.join(FIGURES_DIR, \"cleanlab_analysis\")\n",
        "os.makedirs(CLEANLAB_DIR, exist_ok=True)\n",
        "\n",
        "# Figure 1: Confidence Separation\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_melted = df_cleanlab_combined.melt(id_vars=['experiment'], value_vars=['avg_conf_clean', 'avg_conf_noisy'],\n",
        "                                 var_name='Confidence Type', value_name='Score')\n",
        "conf_melted['Confidence Type'] = conf_melted['Confidence Type'].replace({\n",
        "    'avg_conf_clean': 'Clean Samples',\n",
        "    'avg_conf_noisy': 'Noisy/Issue Samples'\n",
        "})\n",
        "\n",
        "sns.boxplot(x='Confidence Type', y='Score', data=conf_melted, palette='Set2')\n",
        "sns.stripplot(x='Confidence Type', y='Score', data=conf_melted, color=\".3\", alpha=0.5)\n",
        "plt.title('Cleanlab Issue Detection: Predicted Confidence Separation', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Mean Predicted Confidence')\n",
        "plt.xlabel('')\n",
        "save_plot(\"cleanlab\", \"Confidence Separation\", \"01_confidence_separation.png\")\n",
        "\n",
        "# Figure 2: Class-wise Noise Analysis\n",
        "noise_cols = [f'class_noise_{i}' for i in range(1, 6)]\n",
        "heatmap_data = df_cleanlab_combined.set_index('experiment')[noise_cols]\n",
        "heatmap_data.columns = [f'Class {i}' for i in range(1, 6)]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(heatmap_data, annot=True, cmap='YlOrRd', fmt='.3f', cbar_kws={'label': 'Estimated Noise Proportion'})\n",
        "plt.title('Class-Specific Noise Estimated by Cleanlab', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Corruption Experiment')\n",
        "plt.xlabel('Review Rating (Class)')\n",
        "save_plot(\"cleanlab\", \"Class-wise Noise Heatmap\", \"02_class_wise_noise_heatmap.png\")\n",
        "\n",
        "# Figure 3: Reweighting Behavior Analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df_cleanlab_combined, x='issue_rate', y='avg_weight', hue='experiment',\n",
        "                s=150, palette='tab10', alpha=0.8)\n",
        "\n",
        "z = np.polyfit(df_cleanlab_combined['issue_rate'], df_cleanlab_combined['avg_weight'], 1)\n",
        "p = np.poly1d(z)\n",
        "x_range = np.linspace(df_cleanlab_combined['issue_rate'].min(), df_cleanlab_combined['issue_rate'].max(), 100)\n",
        "plt.plot(x_range, p(x_range), \"r--\", alpha=0.5, label='General Trend')\n",
        "\n",
        "plt.title('Sample Reweighting vs. Detected Noise (Issue Rate)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Detected Issue Rate (proportion)')\n",
        "plt.ylabel('Average Sample Weight')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Experiment')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "save_plot(\"cleanlab\", \"Reweighting Behavior Analysis\", \"03_reweighting_behavior_analysis.png\")\n",
        "\n",
        "print(f\"All CleanLab analysis figures saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2DSnuQHP2UT"
      },
      "source": [
        "## Computing and Saving some Relevant Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRSEE3ym8FC2"
      },
      "source": [
        "### Performance Related"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dZJGakWP11m",
        "outputId": "b122fa68-a69a-4de6-8c8a-d9d64dfead95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating Table 1: Corruption Severity...\n",
            "Generating Table 2: Method Effectiveness...\n",
            "Generating Table 3: Interaction Analysis...\n",
            "Generating Table 4: Computational Cost...\n",
            "Generating Table 5: Data Impact Summary...\n",
            "\n",
            "Analysis Tables generated and saved.\n"
          ]
        }
      ],
      "source": [
        "df = results_df.copy()\n",
        "\n",
        "df['total_time'] = df['corrupt_time'] + df['cleaning_time'] + df['eval_time']\n",
        "df['cleaning_name'] = pd.Categorical(df['cleaning_name'], categories=ALL_CLEANING_ORDER, ordered=True)\n",
        "\n",
        "# Table 1: Corruption Severity (Baseline vs Corrupted)\n",
        "print(\"Generating Table 1: Corruption Severity...\")\n",
        "t1 = df[df['cleaning_name'] == '0_Corrupted'][['experiment', 'baseline_acc', 'accuracy']].copy()\n",
        "t1['accuracy_drop'] = t1['baseline_acc'] - t1['accuracy']\n",
        "t1 = t1.rename(columns={'accuracy': 'corrupted_acc'})\n",
        "t1 = t1.sort_values('accuracy_drop', ascending=False).round(3)\n",
        "file_name = os.path.join(STAT_TABLES_DIR, 'table_01_corruption_severity.csv')\n",
        "t1.to_csv(file_name, index=False)\n",
        "\n",
        "# Table 2: Average Cleaning Method Effectiveness\n",
        "print(\"Generating Table 2: Method Effectiveness...\")\n",
        "t2 = df.groupby('cleaning_name')[['accuracy', 'f1', 'recovery_pct']].mean().round(3)\n",
        "file_name = os.path.join(STAT_TABLES_DIR, 'table_02_method_effectiveness.csv')\n",
        "t2.to_csv(file_name)\n",
        "\n",
        "# 3. Table 3: Cleaning x Corruption Interaction\n",
        "print(\"Generating Table 3: Interaction Analysis...\")\n",
        "t3_acc = df.pivot(index='experiment', columns='cleaning_name', values='accuracy')\n",
        "t3_rec = df.pivot(index='experiment', columns='cleaning_name', values='recovery_pct')\n",
        "\n",
        "# Flattening the pivot for a clean CSV structure\n",
        "t3_acc.columns = [f\"{col}_accuracy\" for col in t3_acc.columns]\n",
        "t3_rec.columns = [f\"{col}_recovery\" for col in t3_rec.columns]\n",
        "t3 = pd.concat([t3_acc, t3_rec], axis=1).round(3)\n",
        "file_name = os.path.join(STAT_TABLES_DIR, 'table_03_cleaning_interaction.csv')\n",
        "t3.to_csv(file_name)\n",
        "\n",
        "# 4. Table 4: Computational Cost Summary (Computational Cost Summary)\n",
        "print(\"Generating Table 4: Computational Cost...\")\n",
        "t4 = df.groupby('cleaning_name')[['corrupt_time', 'cleaning_time', 'eval_time', 'total_time']].mean().round(3)\n",
        "file_name = os.path.join(STAT_TABLES_DIR, 'table_04_computational_cost.csv')\n",
        "t4.to_csv(file_name)\n",
        "\n",
        "# 5. Table 5: Data Impact Summary\n",
        "print(\"Generating Table 5: Data Impact Summary...\")\n",
        "t5 = df.groupby('cleaning_name')[['n_before', 'n_after']].mean()\n",
        "t5['pct_data_affected'] = (100 * (t5['n_before'] - t5['n_after']) / t5['n_before']).round(2)\n",
        "t5 = t5.round(0)\n",
        "t5['n_before'] = t5['n_before'].astype(int)\n",
        "t5['n_after'] = t5['n_after'].astype(int)\n",
        "file_name = os.path.join(STAT_TABLES_DIR, 'table_05_data_impact.csv')\n",
        "t5.to_csv(file_name)\n",
        "\n",
        "print(f\"\\nAnalysis Tables generated and saved.\")\n",
        "\n",
        "del df, t1, t2, t3, t3_acc, t3_rec, t4, t5\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZv4Aq4b8FC2"
      },
      "source": [
        "### Data Related"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD8piY35UPVw",
        "outputId": "cd29d0db-079b-4398-e618-8e89bacdbd3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating Table 6: Dataset Size Before vs After Cleaning ...\n",
            "Generating Table 7: Percentage of Data Affected ...\n",
            "Generating Table 8: Corruption Severity vs Data Loss vs Accuracy ...\n",
            "Generating Table 9: Data Retention vs Accuracy (Per Strategy) ...\n",
            "\n",
            "Data Analysis Tables generated and saved.\n"
          ]
        }
      ],
      "source": [
        "df = results_df.copy()\n",
        "\n",
        "def get_n_issues(stats_str):\n",
        "    if pd.isna(stats_str):\n",
        "        return 0\n",
        "    try:\n",
        "        s = stats_str.replace('np.int64(', '').replace('np.float64(', '').replace(')', '')\n",
        "        d = ast.literal_eval(s)\n",
        "        return d.get('n_issues', 0)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "df['n_issues_cl'] = df['stats'].apply(get_n_issues)\n",
        "\n",
        "df['rows_removed'] = df['n_before'] - df['n_after']\n",
        "df['rows_affected'] = np.where(df['cleaning_name'] == '4_ModelAware',\n",
        "                               df['n_issues_cl'],\n",
        "                               df['rows_removed'])\n",
        "\n",
        "df['pct_affected'] = (df['rows_affected'] / df['n_before']) * 100\n",
        "df['retention_pct'] = (df['n_after'] / df['n_before']) * 100\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "# Table 6: Dataset Size Before vs After Cleaning\n",
        "print(\"Generating Table 6: Dataset Size Before vs After Cleaning ...\")\n",
        "t_d1 = df.groupby('cleaning_name').agg({\n",
        "    'n_before': 'mean',\n",
        "    'n_after': 'mean',\n",
        "    'rows_affected': 'mean'\n",
        "}).reset_index()\n",
        "t_d1.rename(columns={'rows_affected': 'rows_affected_or_removed'}, inplace=True)\n",
        "file_name = os.path.join(STAT_TABLES_DIR, 'table_06_size_impact.csv')\n",
        "t_d1.to_csv(file_name, index=False)\n",
        "\n",
        "# Table 7: Percentage of Data Affected\n",
        "print(\"Generating Table 7: Percentage of Data Affected ...\")\n",
        "t_d2 = df.groupby('cleaning_name').agg({\n",
        "    'pct_affected': 'mean'\n",
        "}).reset_index()\n",
        "file_name = os.path.join(STAT_TABLES_DIR, 'table_07_pct_affected.csv')\n",
        "t_d2.to_csv(file_name, index=False)\n",
        "\n",
        "# Table 8: Corruption Severity vs Data Loss vs Accuracy\n",
        "print(\"Generating Table 8: Corruption Severity vs Data Loss vs Accuracy ...\")\n",
        "corruption_impact = df[df['cleaning_name'] == '0_Corrupted'][['experiment', 'accuracy', 'pct_affected']].copy()\n",
        "corruption_impact.rename(columns={'accuracy': 'corrupted_accuracy', 'pct_affected': 'intrinsic_data_loss_pct'}, inplace=True)\n",
        "\n",
        "avg_cleaned_acc = df[df['cleaning_name'] != '0_Corrupted'].groupby('experiment')['accuracy'].mean().reset_index()\n",
        "t_d3 = pd.merge(corruption_impact, avg_cleaned_acc, on='experiment')\n",
        "t_d3.rename(columns={'accuracy': 'avg_cleaned_accuracy'}, inplace=True)\n",
        "file_name = os.path.join(STAT_TABLES_DIR, 'table_08_severity_vs_loss.csv')\n",
        "t_d3.to_csv(file_name, index=False)\n",
        "\n",
        "# Table 9: Data Retention vs Accuracy (Per Strategy)\n",
        "print(\"Generating Table 9: Data Retention vs Accuracy (Per Strategy) ...\")\n",
        "t_d4 = df.groupby('cleaning_name').agg({\n",
        "    'retention_pct': 'mean',\n",
        "    'accuracy': 'mean'\n",
        "}).reset_index()\n",
        "t_d4.sort_values(by='accuracy', ascending=False, inplace=True)\n",
        "file_name = os.path.join(STAT_TABLES_DIR, 'table_09_retention_vs_accuracy.csv')\n",
        "t_d4.to_csv(file_name, index=False)\n",
        "\n",
        "print(f\"\\nData Analysis Tables generated and saved.\")\n",
        "\n",
        "del df, t_d1, t_d2, t_d3, t_d4\n",
        "clear_memory()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
